{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fa3e2af",
   "metadata": {},
   "source": [
    "## Pytorch nn.Conv2d卷积网络使用教程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a431e361",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-06T09:30:52.891811Z",
     "start_time": "2023-04-06T09:30:50.666741Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b33245c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-06T09:30:52.907735Z",
     "start_time": "2023-04-06T09:30:52.894769Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.8740, 0.0408],\n",
      "          [0.8836, 0.6468]]]], grad_fn=<ConvolutionBackward0>)\n",
      "tensor([[[[0.8740, 0.0408],\n",
      "          [0.8836, 0.6468]]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 定义参数\n",
    "in_channels = 1\n",
    "out_channels = 1\n",
    "kernel_size = 3\n",
    "batch_size = 1\n",
    "bias = False\n",
    "input_size = [batch_size, in_channels, 4, 4]\n",
    "\n",
    "# 定义输入特征图\n",
    "input_feature_map = torch.randn(input_size)\n",
    "# 用nn.Conv2d创建卷积层\n",
    "conv_layer = nn.Conv2d(in_channels, out_channels, kernel_size, bias=bias)\n",
    "# 卷积运算\n",
    "output_feature_map = conv_layer(input_feature_map)\n",
    "print(output_feature_map)\n",
    "\n",
    "# 函数接口实现的卷积运算\n",
    "output_feature_map = F.conv2d(input_feature_map, conv_layer.weight)\n",
    "print(output_feature_map)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3b9d7f",
   "metadata": {},
   "source": [
    "## 手写并验证二维卷积"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8d13a1",
   "metadata": {},
   "source": [
    "### step1 滑动窗口实现二维卷积，不考虑batch size维度和channel维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a773536",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-06T09:30:52.939159Z",
     "start_time": "2023-04-06T09:30:52.910726Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(5, 5) # 卷积输入特征图\n",
    "kernel = torch.randn(3, 3) # 卷积核\n",
    "bias = torch.randn(1) # 卷积偏置， 默认输出通道数目等于1\n",
    "\n",
    "def matrix_multiplication_for_conv2d(input, kernel, bias=0, stride=1, padding=0):\n",
    "    if padding > 0:\n",
    "        input = F.pad(input, (padding, padding, padding, padding)) # 左右上下pad\n",
    "    \n",
    "    input_h, input_w = input.shape\n",
    "    kernel_h, kernel_w = kernel.shape\n",
    "    \n",
    "    output_h = math.floor((input_h - kernel_h) / stride) + 1 # 卷积输出的高度\n",
    "    output_w = math.floor((input_w - kernel_w) / stride) + 1 # 卷积输出的宽度\n",
    "    \n",
    "    output = torch.zeros(output_h, output_w) # 初始化输出矩阵\n",
    "    \n",
    "    for i in range(0, input_h - kernel_h + 1, stride): # 对高度维进行遍历\n",
    "        for j in range(0, input_w - kernel_w + 1, stride): # 对宽度维进行遍历\n",
    "            region = input[i:i+kernel_h, j:j+kernel_w] # 取出被核滑动到的区域\n",
    "            output[int(i/stride), int(j/stride)] = torch.sum(region * kernel) + bias # 点乘，并赋值给输出位置的元素\n",
    "    \n",
    "    return output\n",
    "\n",
    "# 矩阵乘法实现卷积的结果\n",
    "mat_mul_conv_output = matrix_multiplication_for_conv2d(input, kernel, bias=bias, padding=1, stride=2)\n",
    "\n",
    "# 调用Pytorch API卷积的结果\n",
    "pytorch_api_conv_output = F.conv2d(input.reshape((1, 1, input.shape[0], input.shape[1])), \\\n",
    "                                   kernel.reshape((1, 1, kernel.shape[0], kernel.shape[1])), \\\n",
    "                                   padding=1, \\\n",
    "                                   bias=bias, \\\n",
    "                                   stride=2).squeeze()\n",
    "# 验证滑动窗口的版本与Pytorch API的结果相同\n",
    "compare_result = torch.allclose(mat_mul_conv_output, pytorch_api_conv_output)\n",
    "print(compare_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7f5779",
   "metadata": {},
   "source": [
    "### step2 向量乘法实现二维卷积，只flatten卷积区域版本，不考虑batch size维度和channel维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afe3cd14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-06T09:30:52.971075Z",
     "start_time": "2023-04-06T09:30:52.942152Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(5, 5) # 卷积输入特征图\n",
    "kernel = torch.randn(3, 3) # 卷积核\n",
    "bias = torch.randn(1) # 卷积偏置， 默认输出通道数目等于1\n",
    "\n",
    "def matrix_multiplication_for_conv2d_flatten(input, kernel, bias=0, stride=1, padding=0):\n",
    "    if padding > 0:\n",
    "        input = F.pad(input, (padding, padding, padding, padding)) # 左右上下pad\n",
    "    \n",
    "    input_h, input_w = input.shape\n",
    "    kernel_h, kernel_w = kernel.shape\n",
    "    \n",
    "    output_h = math.floor((input_h - kernel_h) / stride) + 1 # 卷积输出的高度\n",
    "    output_w = math.floor((input_w - kernel_w) / stride) + 1 # 卷积输出的宽度\n",
    "    \n",
    "    region_matrix = torch.zeros(output_h * output_w, kernel.numel()) # 存储着所有的拉平后特征区域\n",
    "    kernel_matrix = kernel.reshape((-1, 1)) # kernel的列向量（矩阵形式）\n",
    "    \n",
    "    for i in range(0, input_h - kernel_h + 1, stride): # 对高度维进行遍历\n",
    "        for j in range(0, input_w - kernel_w + 1, stride): # 对宽度维进行遍历\n",
    "            region = input[i:i+kernel_h, j:j+kernel_w] # 取出被核滑动到的区域\n",
    "            region_vector = torch.flatten(region)\n",
    "            region_matrix[i//stride * output_w + j//stride] = region_vector\n",
    "    \n",
    "    # 矩阵相乘\n",
    "    output_matrix = region_matrix @ kernel_matrix\n",
    "    output = output_matrix.reshape((output_h, output_w)) + bias\n",
    "    \n",
    "    return output\n",
    "\n",
    "# 矩阵乘法实现卷积的结果，只flatten卷积区域版本\n",
    "mat_mul_conv_output_flatten = matrix_multiplication_for_conv2d_flatten(input, kernel, bias=bias, padding=1, stride=2)\n",
    "\n",
    "# 调用Pytorch API卷积的结果\n",
    "pytorch_api_conv_output = F.conv2d(input.reshape((1, 1, input.shape[0], input.shape[1])), \\\n",
    "                                   kernel.reshape((1, 1, kernel.shape[0], kernel.shape[1])), \\\n",
    "                                   padding=1, \\\n",
    "                                   bias=bias, \\\n",
    "                                   stride=2).squeeze()\n",
    "# 验证矩阵乘法只flatten卷积区域版本与Pytorch API的结果相同\n",
    "compare_result = torch.allclose(mat_mul_conv_output_flatten, pytorch_api_conv_output)\n",
    "print(compare_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bdeb98",
   "metadata": {},
   "source": [
    "### step3 张量乘法实现二维卷积，flatten整个input版本，完整版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e14186b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-06T09:30:53.018945Z",
     "start_time": "2023-04-06T09:30:52.974066Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(4, 3, 5, 5) # 卷积输入特征图\n",
    "kernel = torch.randn(64, 3, 3, 3) # 卷积核\n",
    "bias = torch.randn(64)\n",
    "\n",
    "def matrix_multiplication_for_conv2d_full(input, kernel, bias=None, stride=1, padding=0):\n",
    "    # input, kernel都是4维的张量\n",
    "    if padding > 0:\n",
    "        input = F.pad(input, (padding, padding, padding, padding, 0, 0, 0, 0)) # 左右、上下、channel、batch size\n",
    "    \n",
    "    bs, in_channel, input_h, input_w = input.shape\n",
    "    out_channel, in_channel, kernel_h, kernel_w = kernel.shape\n",
    "    if bias is None:\n",
    "        bias = torch.zeros(out_channel)\n",
    "    \n",
    "    output_h = math.floor((input_h - kernel_h) / stride) + 1 # 卷积输出的高度\n",
    "    output_w = math.floor((input_w - kernel_w) / stride) + 1 # 卷积输出的宽度\n",
    "    \n",
    "    kernel_matrix = torch.zeros((out_channel, in_channel * input_h * input_w, output_h * output_w))\n",
    "    \n",
    "    for k in range(out_channel):\n",
    "        tmp_kernel_matrix = torch.zeros((in_channel, input_h, input_w, output_h * output_w))\n",
    "        for i in range(0, input_h - kernel_h + 1, stride): # 对高度维进行遍历\n",
    "            for j in range(0, input_w - kernel_w + 1, stride): # 对宽度维进行遍历\n",
    "                # 卷积区域填充卷积核参数\n",
    "                tmp_kernel_matrix[:, i:i+kernel_h, j:j+kernel_w, i//stride * output_w + j//stride] = kernel[k, :, :, :]\n",
    "        # 保存\n",
    "        kernel_matrix[k, :, :] = tmp_kernel_matrix.reshape((-1, output_h * output_w))\n",
    "    \n",
    "    # 调整展开后卷积核的维度\n",
    "    full_kernel_matrix = kernel_matrix.repeat(bs, 1, 1, 1)\n",
    "    \n",
    "    # 调整输入特征图的维度，通道、高和宽展开\n",
    "    input_matrix = input.reshape((bs, 1, 1, -1)).repeat(1, out_channel, 1, 1)\n",
    "    \n",
    "    # 矩阵相乘\n",
    "    output = torch.matmul(input_matrix, full_kernel_matrix).reshape((bs, out_channel, output_h, output_w)) + bias.reshape((1, -1, 1, 1))\n",
    "    \n",
    "    return output\n",
    "\n",
    "# 矩阵乘法实现卷积的结果，flatten整个input版本\n",
    "mat_mul_conv_output_full = matrix_multiplication_for_conv2d_full(input, kernel, bias=bias, padding=1, stride=2)\n",
    "\n",
    "# 调用Pytorch API卷积的结果\n",
    "pytorch_api_conv_output = F.conv2d(input, kernel, bias=bias, padding=1, stride=2)\n",
    "\n",
    "# 验证矩阵乘法flatten整个input版本与Pytorch API的结果相同\n",
    "compare_result = torch.allclose(mat_mul_conv_output_full, pytorch_api_conv_output, rtol=0.00001, atol=0.00001)\n",
    "print(compare_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9973d959",
   "metadata": {},
   "source": [
    "## 手写并验证转置卷积"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ba6207",
   "metadata": {},
   "source": [
    "### 转置卷积的张量转置乘法版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b54eef0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-06T09:30:53.065332Z",
     "start_time": "2023-04-06T09:30:53.021938Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "bias = torch.randn(3)\n",
    "\n",
    "def transpose_conv_matrix_multiplication(conv_output, kernel, stride=1, padding=0, output_padding=0, bias=None):\n",
    "    # 获取维度信息\n",
    "    bs, out_channel, output_h, output_w = conv_output.shape\n",
    "    out_channel, in_channel, kernel_h, kernel_w = kernel.shape\n",
    "    # 计算卷积前特征图大小\n",
    "    input_h = (output_h - 1) * stride + kernel_h\n",
    "    input_w = (output_w - 1) * stride + kernel_w\n",
    "    if bias is None:\n",
    "        bias = torch.zeros(in_channel)\n",
    "    \n",
    "    # 初始化卷积核展开后的张量\n",
    "    kernel_matrix = torch.zeros((out_channel, in_channel * input_h * input_w, output_h * output_w))\n",
    "    \n",
    "    for k in range(out_channel):\n",
    "        # 初始化对应于一个输出通道的卷积核展开后的张量\n",
    "        tmp_kernel_matrix = torch.zeros((in_channel, input_h, input_w, output_h * output_w))\n",
    "        for i in range(0, input_h - kernel_h + 1, stride): # 对高度维进行遍历\n",
    "            for j in range(0, input_w - kernel_w + 1, stride): # 对宽度维进行遍历\n",
    "                # 卷积区域填充卷积核参数\n",
    "                tmp_kernel_matrix[:, i:i+kernel_h, j:j+kernel_w, i//stride * output_w + j//stride] = kernel[k, :, :, :]\n",
    "        # 保存\n",
    "        kernel_matrix[k, :, :] = tmp_kernel_matrix.reshape((-1, output_h * output_w))\n",
    "    \n",
    "    # 调整展开后的卷积核张量的维度，对应于转置卷积的转置操作\n",
    "    full_kernel_matrix = kernel_matrix.repeat(bs, 1, 1, 1).reshape((bs, out_channel, in_channel, input_h * input_w, output_h * output_w))\n",
    "    full_kernel_matrix = full_kernel_matrix.permute(0, 2, 1, 4, 3).reshape((bs, in_channel, out_channel * output_h * output_w, input_h * input_w))\n",
    "    \n",
    "    # 调整输出特征图的维度，通道、高和宽展开\n",
    "    output_matrix = conv_output.reshape((bs, 1, 1, -1)).repeat(1, in_channel, 1, 1)\n",
    "    \n",
    "    # 矩阵相乘\n",
    "    input = torch.matmul(output_matrix, full_kernel_matrix).reshape((bs, in_channel, input_h, input_w)) + bias.reshape((1, -1, 1, 1))\n",
    "    \n",
    "    # 根据两类padding调整最终输出特征图的尺寸\n",
    "    final_input = input[:, :, padding:input_h-padding, padding:input_w-padding]\n",
    "    final_input = F.pad(final_input, (0, output_padding, 0, output_padding, 0, 0, 0, 0))\n",
    "    \n",
    "    return final_input\n",
    "\n",
    "# 手写实现的转置卷积\n",
    "my_transpose_conv2d_output = transpose_conv_matrix_multiplication(pytorch_api_conv_output, kernel, stride=2, padding=1, bias=bias)\n",
    "\n",
    "# 调用Pytorch API转置卷积的结果\n",
    "pytorch_transpose_conv2d_output = F.conv_transpose2d(pytorch_api_conv_output, kernel, stride=2, padding=1, bias=bias)\n",
    "\n",
    "# 验证手写实现的转置卷积与Pytorch API的结果相同\n",
    "compare_result = torch.allclose(my_transpose_conv2d_output, pytorch_transpose_conv2d_output, rtol=0.00001, atol=0.00001)\n",
    "print(compare_result)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadc64b9",
   "metadata": {},
   "source": [
    "### 转置卷积的分层叠加版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b89b417c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-06T09:30:53.208054Z",
     "start_time": "2023-04-06T09:30:53.067328Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "bias = torch.randn(3)\n",
    "\n",
    "def transpose_conv_block_stacking(conv_output, kernel, stride=1, padding=0, output_padding=0, bias=None):\n",
    "    # 获取维度信息\n",
    "    bs, out_channel, output_h, output_w = conv_output.shape\n",
    "    out_channel, in_channel, kernel_h, kernel_w = kernel.shape\n",
    "    # 计算卷积前特征图大小\n",
    "    input_h = (output_h - 1) * stride + kernel_h\n",
    "    input_w = (output_w - 1) * stride + kernel_w\n",
    "    if bias is None:\n",
    "        bias = torch.zeros(in_channel)\n",
    "    # 初始化输入特征图\n",
    "    input = torch.zeros((bs, in_channel, input_h, input_w))\n",
    "    \n",
    "    for b in range(bs): # 遍历batch size维度\n",
    "        for oc in range(out_channel): # 遍历输出通道维度\n",
    "            for i in range(0, input_h - kernel_h + 1, stride): # 对高度维进行遍历\n",
    "                for j in range(0, input_w - kernel_w + 1, stride): # 对宽度维进行遍历\n",
    "                    input[b, :, i:i+kernel_h, j:j+kernel_w] += conv_output[b, oc, i//stride, j//stride] * kernel[oc]\n",
    "    \n",
    "    # 加上偏置项\n",
    "    input += bias.reshape((1, -1, 1, 1))\n",
    "    \n",
    "    # 根据两类padding调整最终输出特征图的尺寸\n",
    "    final_input = input[:, :, padding:input_h-padding, padding:input_w-padding]\n",
    "    final_input = F.pad(final_input, (0, output_padding, 0, output_padding, 0, 0, 0, 0))\n",
    "    \n",
    "    return final_input\n",
    "\n",
    "# 手写实现的转置卷积\n",
    "my_transpose_conv2d_output = transpose_conv_block_stacking(pytorch_api_conv_output, kernel, stride=2, padding=1, bias=bias)\n",
    "\n",
    "# 调用Pytorch API转置卷积的结果\n",
    "pytorch_transpose_conv2d_output = F.conv_transpose2d(pytorch_api_conv_output, kernel, stride=2, padding=1, bias=bias)\n",
    "\n",
    "# 验证手写实现的转置卷积与Pytorch API的结果相同\n",
    "compare_result = torch.allclose(my_transpose_conv2d_output, pytorch_transpose_conv2d_output, rtol=0.00001, atol=0.00001)\n",
    "print(compare_result)\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070373d8",
   "metadata": {},
   "source": [
    "### 转置卷积的卷积版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa9c68d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-06T09:30:53.237974Z",
     "start_time": "2023-04-06T09:30:53.211047Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "bias = torch.randn(3)\n",
    "\n",
    "def transpose_conv_conv(conv_output, kernel, stride=1, padding=0, output_padding=0, bias=None):\n",
    "    # 获取维度信息\n",
    "    bs, out_channel, output_h, output_w = conv_output.shape\n",
    "    out_channel, in_channel, kernel_h, kernel_w = kernel.shape\n",
    "    # 计算中间构造的进行卷积的特征图大小\n",
    "    map_h = (output_h - 1) * stride + 1\n",
    "    map_w = (output_w - 1) * stride + 1\n",
    "    # 计算填充大小\n",
    "    p_h = kernel_h - 1 - padding\n",
    "    p_w = kernel_w - 1 - padding\n",
    "    if bias is None:\n",
    "        bias = torch.zeros(in_channel)\n",
    "    \n",
    "    # 构造进行卷积的特征图\n",
    "    feature_map = torch.zeros((bs, out_channel, map_h, map_w))\n",
    "    # 用输出特征图填充构造的特征图\n",
    "    feature_map[:, :, ::stride, ::stride] = conv_output\n",
    "    \n",
    "    # 定义卷积\n",
    "    conv2d_layer = nn.Conv2d(out_channel, in_channel, kernel_size=kernel_h, stride=1, padding=p_h, bias=True)\n",
    "    # kernel后两维数据翻转\n",
    "    weight = torch.flip(kernel, [2, 3])\n",
    "    # 替换卷积中的权重参数\n",
    "    conv2d_layer.weight = nn.Parameter(weight.permute(1, 0, 2, 3))\n",
    "    conv2d_layer.bias = nn.Parameter(bias)\n",
    "    \n",
    "    # 卷积操作\n",
    "    final_input = conv2d_layer(feature_map)\n",
    "    \n",
    "    # 根据两类padding调整最终输出特征图的尺寸\n",
    "    final_input = F.pad(final_input, (0, output_padding, 0, output_padding, 0, 0, 0, 0))\n",
    "    \n",
    "    return final_input\n",
    "\n",
    "# 手写实现的转置卷积\n",
    "my_transpose_conv2d_output = transpose_conv_conv(pytorch_api_conv_output, kernel, stride=2, padding=1, bias=bias)\n",
    "\n",
    "# 调用Pytorch API转置卷积的结果\n",
    "pytorch_transpose_conv2d_output = F.conv_transpose2d(pytorch_api_conv_output, kernel, stride=2, padding=1, bias=bias)\n",
    "\n",
    "# 验证手写实现的转置卷积与Pytorch API的结果相同\n",
    "compare_result = torch.allclose(my_transpose_conv2d_output, pytorch_transpose_conv2d_output, rtol=0.00001, atol=0.00001)\n",
    "print(compare_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2506bf",
   "metadata": {},
   "source": [
    "## 空洞卷积(DilatedConv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adc5a169",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-06T09:30:53.285845Z",
     "start_time": "2023-04-06T09:30:53.239970Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(4, 3, 5, 5) # 卷积输入特征图\n",
    "kernel = torch.randn(64, 3, 3, 3) # 卷积核\n",
    "bias = torch.randn(64)\n",
    "\n",
    "def matrix_multiplication_for_conv2d_full_with_dilation(input, kernel, bias=None, stride=1, padding=0, dilation=1):\n",
    "    # input, kernel都是4维的张量\n",
    "    if padding > 0:\n",
    "        input = F.pad(input, (padding, padding, padding, padding, 0, 0, 0, 0)) # 左右、上下、channel、batch size\n",
    "    \n",
    "    bs, in_channel, input_h, input_w = input.shape\n",
    "    out_channel, in_channel, kernel_h, kernel_w = kernel.shape\n",
    "    if bias is None:\n",
    "        bias = torch.zeros(out_channel)\n",
    "    # 计算填充空洞后的虚拟卷积核大小\n",
    "    virtual_kernel_h = (kernel_h - 1) * dilation + 1\n",
    "    virtual_kernel_w = (kernel_w - 1) * dilation + 1\n",
    "    # 计算卷积后的特征图大小\n",
    "    output_h = math.floor((input_h - virtual_kernel_h) / stride) + 1 # 卷积输出的高度\n",
    "    output_w = math.floor((input_w - virtual_kernel_w) / stride) + 1 # 卷积输出的宽度\n",
    "    # 初始化卷积核展开后的张量\n",
    "    kernel_matrix = torch.zeros((out_channel, in_channel * input_h * input_w, output_h * output_w))\n",
    "    \n",
    "    for k in range(out_channel):\n",
    "        tmp_kernel_matrix = torch.zeros((in_channel, input_h, input_w, output_h * output_w))\n",
    "        for i in range(0, input_h - virtual_kernel_h + 1, stride): # 对高度维进行遍历\n",
    "            for j in range(0, input_w - virtual_kernel_w + 1, stride): # 对宽度维进行遍历\n",
    "                # 卷积区域填充卷积核参数\n",
    "                tmp_kernel_matrix[:, i:i+virtual_kernel_h:dilation, j:j+virtual_kernel_w:dilation, i//stride * output_w + j//stride] = kernel[k, :, :, :]\n",
    "        # 保存\n",
    "        kernel_matrix[k, :, :] = tmp_kernel_matrix.reshape((-1, output_h * output_w))\n",
    "    \n",
    "    # 调整展开后卷积核的维度\n",
    "    full_kernel_matrix = kernel_matrix.repeat(bs, 1, 1, 1)\n",
    "    \n",
    "    # 调整输入特征图的维度，通道、高和宽展开\n",
    "    input_matrix = input.reshape((bs, 1, 1, -1)).repeat(1, out_channel, 1, 1)\n",
    "    \n",
    "    # 矩阵相乘\n",
    "    output = torch.matmul(input_matrix, full_kernel_matrix).reshape((bs, out_channel, output_h, output_w)) + bias.reshape((1, -1, 1, 1))\n",
    "    \n",
    "    return output\n",
    "\n",
    "# 矩阵乘法实现卷积的结果，flatten整个input版本\n",
    "mat_mul_conv_output_full = matrix_multiplication_for_conv2d_full_with_dilation(input, kernel, bias=bias, padding=2, stride=2, dilation=2)\n",
    "\n",
    "# 调用Pytorch API卷积的结果\n",
    "pytorch_api_conv_output = F.conv2d(input, kernel, bias=bias, padding=2, stride=2, dilation=2)\n",
    "\n",
    "# 验证矩阵乘法flatten整个input版本与Pytorch API的结果相同\n",
    "compare_result = torch.allclose(mat_mul_conv_output_full, pytorch_api_conv_output, rtol=0.00001, atol=0.00001)\n",
    "print(compare_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a15a9c",
   "metadata": {},
   "source": [
    "## 群组卷积(GroupConv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc15adb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-06T09:37:24.515990Z",
     "start_time": "2023-04-06T09:37:24.457175Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(4, 4, 5, 5) # 卷积输入特征图\n",
    "kernel = torch.randn(64, 2, 3, 3) # 卷积核\n",
    "bias = torch.randn(64)\n",
    "\n",
    "def matrix_multiplication_for_conv2d_full_with_dilation_and_groups(input, kernel, bias=None, stride=1, padding=0, dilation=1, groups=1):\n",
    "    # input, kernel都是4维的张量\n",
    "    if padding > 0:\n",
    "        input = F.pad(input, (padding, padding, padding, padding, 0, 0, 0, 0)) # 左右、上下、channel、batch size\n",
    "    \n",
    "    bs, in_channel, input_h, input_w = input.shape\n",
    "    out_channel, group_in_channel, kernel_h, kernel_w = kernel.shape\n",
    "    if bias is None:\n",
    "        bias = torch.zeros(out_channel)\n",
    "    \n",
    "    # 计算填充空洞后的虚拟卷积核大小\n",
    "    virtual_kernel_h = (kernel_h - 1) * dilation + 1\n",
    "    virtual_kernel_w = (kernel_w - 1) * dilation + 1\n",
    "    # 计算输入通道和输出通道每组的大小\n",
    "    assert in_channel % groups == 0 and out_channel % groups == 0, \"groups can not mod channel\"   \n",
    "    group_in_channel = in_channel // groups\n",
    "    group_out_channel = out_channel // groups\n",
    "    # 计算卷积后的特征图大小\n",
    "    output_h = math.floor((input_h - virtual_kernel_h) / stride) + 1 # 卷积输出的高度\n",
    "    output_w = math.floor((input_w - virtual_kernel_w) / stride) + 1 # 卷积输出的宽度\n",
    "    # 初始化卷积核展开后的张量\n",
    "    kernel_matrix = torch.zeros((out_channel, in_channel * input_h * input_w, output_h * output_w))\n",
    "    \n",
    "    for g in range(0, out_channel, group_out_channel):\n",
    "        # 计算当前分组的输出通道起始下标\n",
    "        out_channel_start_ind = g\n",
    "        out_channel_end_ind = g + group_out_channel\n",
    "        # 计算当前分组的输入通道起始下标\n",
    "        in_channel_start_ind = (g // group_out_channel) * group_in_channel\n",
    "        in_channel_end_ind = in_channel_start_ind + group_in_channel\n",
    "        for k in range(out_channel_start_ind, out_channel_end_ind):\n",
    "            tmp_kernel_matrix = torch.zeros((in_channel, input_h, input_w, output_h * output_w))\n",
    "            for i in range(0, input_h - virtual_kernel_h + 1, stride): # 对高度维进行遍历\n",
    "                for j in range(0, input_w - virtual_kernel_w + 1, stride): # 对宽度维进行遍历\n",
    "                    # 卷积区域填充卷积核参数\n",
    "                    tmp_kernel_matrix[in_channel_start_ind:in_channel_end_ind, i:i+virtual_kernel_h:dilation, j:j+virtual_kernel_w:dilation, i//stride * output_w + j//stride] = kernel[k, :, :, :]\n",
    "            # 保存\n",
    "            kernel_matrix[k, :, :] = tmp_kernel_matrix.reshape((-1, output_h * output_w))\n",
    "    \n",
    "    # 调整展开后卷积核的维度\n",
    "    full_kernel_matrix = kernel_matrix.repeat(bs, 1, 1, 1)\n",
    "    \n",
    "    # 调整输入特征图的维度，通道、高和宽展开\n",
    "    input_matrix = input.reshape((bs, 1, 1, -1)).repeat(1, out_channel, 1, 1)\n",
    "    \n",
    "    # 矩阵相乘\n",
    "    output = torch.matmul(input_matrix, full_kernel_matrix).reshape((bs, out_channel, output_h, output_w)) + bias.reshape((1, -1, 1, 1))\n",
    "    \n",
    "    return output\n",
    "\n",
    "# 矩阵乘法实现卷积的结果，flatten整个input版本\n",
    "mat_mul_conv_output_full = matrix_multiplication_for_conv2d_full_with_dilation_and_groups(input, kernel, bias=bias, padding=2, stride=2, dilation=2, groups=2)\n",
    "\n",
    "# 调用Pytorch API卷积的结果\n",
    "pytorch_api_conv_output = F.conv2d(input, kernel, bias=bias, padding=2, stride=2, dilation=2, groups=2)\n",
    "\n",
    "# 验证矩阵乘法flatten整个input版本与Pytorch API的结果相同\n",
    "compare_result = torch.allclose(mat_mul_conv_output_full, pytorch_api_conv_output, rtol=0.00001, atol=0.00001)\n",
    "print(compare_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b29cee5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "406.062px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
